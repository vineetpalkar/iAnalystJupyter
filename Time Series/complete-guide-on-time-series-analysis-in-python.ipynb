{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# **Complete Guide on Time Series Analysis in Python**\n\n\n\nHello friends,\n\n\nAs the name implies, this notebook is all about **Time Series Analysis**. A time series is a series of data points recorded at different time-intervals. The time series analysis means analyzing the time series data using various statistical tools and techniques. \n\nSo, let's get started.","metadata":{}},{"cell_type":"markdown","source":"### **I hope you find this notebook useful and your <font color=\"red\"><b>UPVOTES</b></font> keep me motivated.**","metadata":{"trusted":true}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n# **Table of Contents**\n\n\n1.\t[Introduction to Time Series Analysis](#1)\n2.\t[Types of data](#2)\n3.\t[Time Series terminology](#3)\n4.\t[Time Series Analysis](#4)\n5.\t[Visualize the Time Series](#5)\n6.\t[Patterns in a Time Series](#6)\n7.\t[Additive and Multiplicative Time Series](#7)\n8.\t[Decomposition of a Time Series](#8)\n9.\t[Stationary and Non-Stationary Time Series](#9)\n10.\t[How to make a time series stationary](#10)\n11.\t[How to test for stationarity](#11)\n    - 11.1\t[Augmented Dickey Fuller test (ADF Test)](#11.1)\n    - 11.2\t[Kwiatkowski-Phillips-Schmidt-Shin â€“ KPSS test (trend stationary)](#11.2)\n    - 11.3\t[Philips Perron test (PP Test)](#11.3)\n12.\t[Difference between white noise and a stationary series](#12)\n13.\t[Detrend a Time Series](#13)\n14.\t[Deseasonalize a Time Series](#14)\n15.\t[How to test for seasonality of a time series](#15)\n16.\t[Autocorrelation and Partial Autocorrelation Functions](#16)\n17.\t[Computation of Partial Autocorrelation Function](#17)\n18.\t[Lag Plots](#18)\n19.\t[Granger Causality Test](#19)\n20.\t[Smoothening a Time Series](#20)\n21.\t[References](#21)\n","metadata":{}},{"cell_type":"markdown","source":"# **1. Introduction to Time-Series Analysis** <a class=\"anchor\" id=\"1\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n\n- A **time-series** data is a series of data points or observations recorded at different or regular time intervals. In general, a time series is a sequence of data points taken at equally spaced time intervals.  The frequency of recorded data points may be hourly, daily, weekly, monthly, quarterly or annually.\n\n\n- **Time-Series Forecasting** is the process of using a statistical model to predict future values of a time-series based on past results.\n\n\n- A time series analysis encompasses statistical methods for analyzing time series data. These methods enable us to extract meaningful statistics, patterns and other characteristics of the data. Time series are visualized with the help of line charts. So, time series analysis involves understanding inherent aspects of the time series data so that we can create meaningful and accurate forecasts.\n\n\n- Applications of time series are used in statistics, finance or business applications. A very common example of time series data is the daily closing value of the stock index like NASDAQ or Dow Jones. Other common applications of time series are sales and demand forecasting, weather forecasting, econometrics, signal processing, pattern recognition and earthquake prediction.\n\n\n\n### **Components of a Time-Series**\n\n\n- **Trend** - The trend shows a general direction of the time series data over a long period of time. A trend can be increasing(upward), decreasing(downward), or horizontal(stationary).\n\n\n- **Seasonality** - The seasonality component exhibits a trend that repeats with respect to timing, direction, and magnitude. Some examples include an increase in water consumption in summer due to hot weather conditions.\n\n\n- **Cyclical Component** - These are the trends with no set repetition over a particular period of time. A cycle refers to the period of ups and downs, booms and slums of a time series, mostly observed in business cycles. These cycles do not exhibit a seasonal variation but generally occur over a time period of 3 to 12 years depending on the nature of the time series.\n\n\n- **Irregular Variation** - These are the fluctuations in the time series data which become evident when trend and cyclical variations are removed. These variations are unpredictable, erratic, and may or may not be random.\n\n\n- **ETS Decomposition** - ETS Decomposition is used to separate different components of a time series. The term ETS stands for Error, Trend and Seasonality.\n\n\n- In this notebook, I conduct time series analysis of video game sales over time.","metadata":{}},{"cell_type":"markdown","source":"# **2. Types of data** <a class=\"anchor\" id=\"2\"></a>\n\n[Table of Contents](#0.1)\n\n\nAs stated above, the time series analysis is the statistical analysis of the time series data. A time series data means that data is recorded at different time periods or intervals. The time series data may be of three types:-\n\n\n1 **Time series data** - The observations of the values of a variable recorded at different points in time is called time series data. \n\n\n2 **Cross sectional data** - It is the data of one or more variables recorded at the same point in time.\n\n\n3 **Pooled data**- It is the combination of time series data and cross sectional data.\n","metadata":{}},{"cell_type":"markdown","source":"# **3. Time Series terminology** <a class=\"anchor\" id=\"3\"></a>\n\n[Table of Contents](#0.1)\n\n\nThere are various terms and concepts in time series that we should know. These are as follows:-\n\n1\t**Dependence**- It refers to the association of two observations of the same variable at prior time periods.\n\n\n2\t**Stationarity**- It shows the mean value of the series that remains constant over the time period. If past effects accumulate and the values increase towards infinity then stationarity is not met.\n\n\n3\t**Differencing**- Differencing is used to make the series stationary and to control the auto-correlations. There may be some cases in time series analyses where we do not require differencing and over-differenced series can produce wrong estimates.\n\n\n4\t**Specification** - It may involve the testing of the linear or non-linear relationships of dependent variables by using time series models such as ARIMA models. \n\n\n5\t**Exponential Smoothing** - Exponential smoothing in time series analysis predicts the one next period value based on the past and current value.  It involves averaging of data such that the non-systematic components of each individual case or observation cancel out each other.  The exponential smoothing method is used to predict the short term prediction.\n\n\n\n6\t**Curve fitting** - Curve fitting regression in time series analysis is used when data is in a non-linear relationship.\n\n\n7\t**ARIMA** - ARIMA stands for Auto Regressive Integrated Moving Average.\n","metadata":{}},{"cell_type":"markdown","source":"# **4. Time Series Analysis** <a class=\"anchor\" id=\"4\"></a>\n\n[Table of Contents](#0.1)\n","metadata":{}},{"cell_type":"markdown","source":"## **4.1 Basic set up** <a class=\"anchor\" id=\"4.1\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt   # data visualization\nimport seaborn as sns             # statistical data visualization\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4.2 Import data** <a class=\"anchor\" id=\"4.2\"></a>\n\n[Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/air-passengers/AirPassengers.csv'\n\ndf = pd.read_csv(path)\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We should rename the column names.","metadata":{}},{"cell_type":"code","source":"df.columns = ['Date','Number of Passengers']\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Visualize the Time Series** <a class=\"anchor\" id=\"5\"></a>\n\n\n[Table of Contents](#0.1)\n","metadata":{}},{"cell_type":"code","source":"def plot_df(df, x, y, title=\"\", xlabel='Date', ylabel='Number of Passengers', dpi=100):\n    plt.figure(figsize=(15,4), dpi=dpi)\n    plt.plot(x, y, color='tab:red')\n    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n    plt.show()\n    \n\nplot_df(df, x=df['Date'], y=df['Number of Passengers'], title='Number of US Airline passengers from 1949 to 1960')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Since all the values are positive, we can show this on both sides of the Y axis to emphasize the growth.","metadata":{}},{"cell_type":"code","source":"x = df['Date'].values\ny1 = df['Number of Passengers'].values\n\n# Plot\nfig, ax = plt.subplots(1, 1, figsize=(16,5), dpi= 120)\nplt.fill_between(x, y1=y1, y2=-y1, alpha=0.5, linewidth=2, color='seagreen')\nplt.ylim(-800, 800)\nplt.title('Air Passengers (Two Side View)', fontsize=16)\nplt.hlines(y=0, xmin=np.min(df['Date']), xmax=np.max(df['Date']), linewidth=.5)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It can be seen that its a monthly time series and follows a certain repetitive pattern every year. So, we can plot each year as a separate line in the same plot. This let us compare the year wise patterns side-by-side.","metadata":{}},{"cell_type":"markdown","source":"# **6. Patterns in a Time Series** <a class=\"anchor\" id=\"6\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Any time series visualization may consist of the following components: **Base Level + Trend + Seasonality + Error**.\n\n\n### **Trend**\n\n- A **trend** is observed when there is an increasing or decreasing slope observed in the time series. \n\n\n### **Seasonality**\n\n- A **seasonality** is observed when there is a distinct repeated pattern observed between regular intervals due to seasonal factors. It could be because of the month of the year, the day of the month, weekdays or even time of the day.\n\n\nHowever, It is not mandatory that all time series must have a trend and/or seasonality. A time series may not have a distinct trend but have a seasonality and vice-versa.\n","metadata":{}},{"cell_type":"code","source":"def plot_df(df, x, y, title=\"\", xlabel='Date', ylabel='Number of Passengers', dpi=100):\n    plt.figure(figsize=(15,4), dpi=dpi)\n    plt.plot(x, y, color='blue')\n    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n    plt.show()\n    \n\nplot_df(df, x=df['Date'], y=df['Number of Passengers'], title='Trend and Seasonality')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Cyclic behaviour**\n\n- Another important thing to consider is the **cyclic behaviour**. It happens when the rise and fall pattern in the series does not happen in fixed calendar-based intervals. We should not confuse 'cyclic' effect with 'seasonal' effect.\n\n- If the patterns are not of fixed calendar based frequencies, then it is cyclic. Because, unlike the seasonality, cyclic effects are typically influenced by the business and other socio-economic factors.","metadata":{}},{"cell_type":"markdown","source":"# **7. Additive and Multiplicative Time Series** <a class=\"anchor\" id=\"7\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- We may have different combinations of trends and seasonality. Depending on the nature of the trends and seasonality, a time series can be modeled as an additive or multiplicative time series. Each observation in the series can be expressed as either a sum or a product of the components.\n\n\n### **Additive time series:**\n\nValue = Base Level + Trend + Seasonality + Error\n\n\n### **Multiplicative Time Series:**\n\nValue = Base Level x Trend x Seasonality x Error","metadata":{}},{"cell_type":"markdown","source":"# **8. Decomposition of a Time Series** <a class=\"anchor\" id=\"8\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Decomposition of a time series can be performed by considering the series as an additive or multiplicative combination of the base level, trend, seasonal index and the residual term.\n\n\n- The seasonal_decompose in statsmodels implements this conveniently.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom dateutil.parser import parse\n\n\n# Multiplicative Decomposition \nmultiplicative_decomposition = seasonal_decompose(df['Number of Passengers'], model='multiplicative', period=30)\n\n# Additive Decomposition\nadditive_decomposition = seasonal_decompose(df['Number of Passengers'], model='additive', period=30)\n\n# Plot\nplt.rcParams.update({'figure.figsize': (16,12)})\nmultiplicative_decomposition.plot().suptitle('Multiplicative Decomposition', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\nadditive_decomposition.plot().suptitle('Additive Decomposition', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- If we look at the residuals of the additive decomposition closely, it has some pattern left over. \n\n- The multiplicative decomposition, looks quite random which is good. So ideally, multiplicative decomposition should be preferred for this particular series.","metadata":{}},{"cell_type":"markdown","source":"# **9. Stationary and Non-Stationary Time Series** <a class=\"anchor\" id=\"9\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Now, we wil discuss **Stationary and Non-Stationary Time Series**. **Stationarity** is a property of a time series. A stationary series is one where the values of the series is not a function of time. So, the values are independent of time.\n\n\n- Hence the statistical properties of the series like mean, variance and autocorrelation are constant over time. Autocorrelation of the series is nothing but the correlation of the series with its previous values.\n\n\n- A stationary time series is independent of seasonal effects as well.\n\n\n- Now, we will plot some examples of stationary and non-stationary time series for clarity.","metadata":{}},{"cell_type":"markdown","source":"![Stationary and Non-Stationary Time Series](https://www.machinelearningplus.com/wp-content/uploads/2019/02/stationary-and-non-stationary-time-series-865x569.png?ezimgfmt=ng:webp/ngcb1)\n\nimage source : https://www.machinelearningplus.com/wp-content/uploads/2019/02/stationary-and-non-stationary-time-series-865x569.png?ezimgfmt=ng:webp/ngcb1","metadata":{}},{"cell_type":"markdown","source":"- We can covert any non-stationary time series into a stationary one by applying a suitable transformation. Mostly statistical forecasting methods are designed to work on a stationary time series. The first step in the forecasting process is typically to do some transformation to convert a non-stationary series to stationary.","metadata":{}},{"cell_type":"markdown","source":"\n\n# **10. How to make a time series stationary?** <a class=\"anchor\" id=\"10\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- We can apply some sort of transformation to make the time-series stationary. These transformation may include:\n\n\n1. Differencing the Series (once or more)\n2. Take the log of the series\n3. Take the nth root of the series\n4. Combination of the above\n\n\n- The most commonly used and convenient method to stationarize the series is by differencing the series at least once until it becomes approximately stationary.","metadata":{}},{"cell_type":"markdown","source":"## **10.1 Introduction to Differencing** <a class=\"anchor\" id=\"10.1\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- If Y_t is the value at time t, then the first difference of Y = Yt â€“ Yt-1. In simpler terms, differencing the series is nothing but subtracting the next value by the current value.\n\n\n- If the first difference doesnâ€™t make a series stationary, we can go for the second differencing and so on.\n\n\n  - For example, consider the following series: [1, 5, 2, 12, 20]\n\n\n  - First differencing gives: [5-1, 2-5, 12-2, 20-12] = [4, -3, 10, 8]\n\n\n  - Second differencing gives: [-3-4, -10-3, 8-10] = [-7, -13, -2]","metadata":{}},{"cell_type":"markdown","source":"## **10.2 Reasons to convert a non-stationary series into stationary one before forecasting** <a class=\"anchor\" id=\"10.2\"></a>\n\n\n[Table of Contents](#0.1)\n\n\nThere are reasons why we want to convert a non-stationary series into a stationary one. These are given below:\n\n\n- Forecasting a stationary series is relatively easy and the forecasts are more reliable.\n\n\n- An important reason is, autoregressive forecasting models are essentially linear regression models that utilize the lag(s) of the series itself as predictors.\n\n\n- We know that linear regression works best if the predictors (X variables) are not correlated against each other. So, stationarizing the series solves this problem since it removes any persistent autocorrelation, thereby making the predictors(lags of the series) in the forecasting models nearly independent.","metadata":{}},{"cell_type":"markdown","source":"# **11. How to test for stationarity?** <a class=\"anchor\" id=\"11\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- The stationarity of a series can be checked by looking at the plot of the series.\n\n\n- Another method is to split the series into 2 or more contiguous parts and computing the summary statistics like the mean, variance and the autocorrelation. If the stats are quite different, then the series is not likely to be stationary.\n\n\n- There are several quantitative methods we can use to determine if a given series is stationary or not. This can be done using statistical tests called [Unit Root Tests](https://en.wikipedia.org/wiki/Unit_root). This test checks if a time series is non-stationary and possess a unit root. \n\n\n- There are multiple implementations of Unit Root tests like:\n\n\n**1. Augmented Dickey Fuller test (ADF Test)**\n\n**2. Kwiatkowski-Phillips-Schmidt-Shin â€“ KPSS test (trend stationary)**\n\n**3. Philips Perron test (PP Test)**\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## **11.1 Augmented Dickey Fuller test (ADF Test)** <a class=\"anchor\" id=\"11.1\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- **Augmented Dickey Fuller test or (ADF Test)** is the most commonly used test to detect stationarity. Here, we assume that the null hypothesis is the time series possesses a unit root and is non-stationary. Then, we collect evidence to support or reject the null hypothesis. So, if we find that the p-value in ADF test is less than the significance level (0.05), we reject the null hypothesis.\n\n\n- Feel free to check the following links to learn more about the ADF Test.\n\n\nhttps://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\n\nhttps://www.machinelearningplus.com/time-series/augmented-dickey-fuller-test/\n\nhttps://machinelearningmastery.com/time-series-data-stationary-python/\n\nhttp://www.insightsbot.com/augmented-dickey-fuller-test-in-python/\n\nhttps://nwfsc-timeseries.github.io/atsa-labs/sec-boxjenkins-aug-dickey-fuller.html\n\nhttps://www.statisticshowto.com/adf-augmented-dickey-fuller-test/\n\n","metadata":{}},{"cell_type":"markdown","source":"\n## **11.2 Kwiatkowski-Phillips-Schmidt-Shin â€“ KPSS test (trend stationary)** <a class=\"anchor\" id=\"11.2\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- The KPSS test, on the other hand, is used to test for trend stationarity. The null hypothesis and the P-Value interpretation is just the opposite of ADH test.\n\n- Interested readers can learn more about the KPSS test from the below links:\n\n\nhttps://en.wikipedia.org/wiki/KPSS_test\n\nhttps://www.machinelearningplus.com/time-series/kpss-test-for-stationarity/\n\nhttps://www.statisticshowto.com/kpss-test/\n\nhttps://nwfsc-timeseries.github.io/atsa-labs/sec-boxjenkins-kpss.html\n\n","metadata":{}},{"cell_type":"markdown","source":"## **11.3 Philips Perron test (PP Test)** <a class=\"anchor\" id=\"11.3\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- The Philips Perron or PP test is a [unit root test](https://en.wikipedia.org/wiki/Unit_root). It is used in the [time series analysis](https://en.wikipedia.org/wiki/Time_series) to test the [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis) that a time series is integrated of order 1. It is built on the ADF test discussed above.\n\n\n- For more information on PP test, please visit the following links:\n\n\nhttps://en.wikipedia.org/wiki/Phillips%E2%80%93Perron_test\n\nhttps://www.mathworks.com/help/econ/pptest.html\n\nhttps://people.bath.ac.uk/hssjrh/Phillips%20Perron.pdf\n\nhttps://www.stata.com/manuals13/tspperron.pdf","metadata":{}},{"cell_type":"markdown","source":"# **12. Difference between white noise and a stationary series** <a class=\"anchor\" id=\"12\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Like a stationary series, the white noise is also not a function of time. So, its mean and variance does not change over time. But the difference is that, the white noise is completely random with a mean of 0. In white noise there is no pattern.\n\n- Mathematically, a sequence of completely random numbers with mean zero is a white noise.","metadata":{}},{"cell_type":"code","source":"rand_numbers = np.random.randn(1000)\npd.Series(rand_numbers).plot(title='Random White Noise', color='b')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **13. Detrend a Time Series** <a class=\"anchor\" id=\"13\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- Detrending a time series means to remove the trend component from the time series. There are multiple approaches of doing this as listed below:\n\n\n1. Subtract the line of best fit from the time series. The line of best fit may be obtained from a linear regression model with the time steps as the predictor. For more complex trends, we may want to use quadratic terms (x^2) in the model.\n\n2. We subtract the trend component obtained from time series decomposition.\n\n3. Subtract the mean.\n\n4. Apply a filter like Baxter-King filter(statsmodels.tsa.filters.bkfilter) or the Hodrick-Prescott Filter (statsmodels.tsa.filters.hpfilter) to remove the moving average trend lines or the cyclical components.\n\n\nNow, we will implement the first two methods to detrend a time series.","metadata":{}},{"cell_type":"code","source":"# Using scipy: Subtract the line of best fit\nfrom scipy import signal\ndetrended = signal.detrend(df['Number of Passengers'].values)\nplt.plot(detrended)\nplt.title('Air Passengers detrended by subtracting the least squares fit', fontsize=16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using statmodels: Subtracting the Trend Component\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nresult_mul = seasonal_decompose(df['Number of Passengers'], model='multiplicative', period=30)\ndetrended = df['Number of Passengers'].values - result_mul.trend\nplt.plot(detrended)\nplt.title('Air Passengers detrended by subtracting the trend component', fontsize=16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **14. Deseasonalize a Time Series** <a class=\"anchor\" id=\"14\"></a>\n\n\n[Table of Contents](#0.1)\n\n\nThere are multiple approaches to deseasonalize a time series. These approaches are listed below:\n\n\n- 1. Take a moving average with length as the seasonal window. This will smoothen in series in the process.\n\n- 2. Seasonal difference the series (subtract the value of previous season from the current value).\n\n- 3. Divide the series by the seasonal index obtained from STL decomposition.\n\n\n\nIf dividing by the seasonal index does not work well, we will take a log of the series and then do the deseasonalizing. We will later restore to the original scale by taking an exponential.\n","metadata":{}},{"cell_type":"code","source":"# Subtracting the Trend Component\n\n\n# Time Series Decomposition\nresult_mul = seasonal_decompose(df['Number of Passengers'], model='multiplicative', period=30)\n\n\n# Deseasonalize\ndeseasonalized = df['Number of Passengers'].values / result_mul.seasonal\n\n\n# Plot\nplt.plot(deseasonalized)\nplt.title('Air Passengers Deseasonalized', fontsize=16)\nplt.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **15. How to test for seasonality of a time series?** <a class=\"anchor\" id=\"15\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n\nThe common way to test for seasonality of a time series is to plot the series and check for repeatable patterns in fixed time intervals. So, the types of seasonality is determined by the clock or the calendar.\n\n\n1. Hour of day\n2. Day of month\n3. Weekly\n4. Monthly\n5. Yearly\n\nHowever, if we want a more definitive inspection of the seasonality, use the **Autocorrelation Function (ACF) plot**. There is a strong seasonal pattern, the ACF plot usually reveals definitive repeated spikes at the multiples of the seasonal window.","metadata":{}},{"cell_type":"code","source":"# Test for seasonality\nfrom pandas.plotting import autocorrelation_plot\n\n# Draw Plot\nplt.rcParams.update({'figure.figsize':(10,6), 'figure.dpi':120})\nautocorrelation_plot(df['Number of Passengers'].tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alternately, if we want a statistical test, the [CHTest](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.CHTest.html#pmdarima.arima.CHTest) can determine if seasonal differencing is required to stationarize the series.","metadata":{}},{"cell_type":"markdown","source":"# **16. Autocorrelation and Partial Autocorrelation Functions** <a class=\"anchor\" id=\"16\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- **Autocorrelation** is simply the correlation of a series with its own lags. If a series is significantly autocorrelated, that means, the previous values of the series (lags) may be helpful in predicting the current value.\n\n\n- **Partial Autocorrelation** also conveys similar information but it conveys the pure correlation of a series and its lag, excluding the correlation contributions from the intermediate lags.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Draw Plot\nfig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\nplot_acf(df['Number of Passengers'].tolist(), lags=50, ax=axes[0])\nplot_pacf(df['Number of Passengers'].tolist(), lags=50, ax=axes[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **17. Computation of Partial Autocorrelation Function** <a class=\"anchor\" id=\"17\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- The partial autocorrelation function of lag (k) of a series is the coefficient of that lag in the autoregression equation of Y. The autoregressive equation of Y is nothing but the linear regression of Y with its own lags as predictors.\n\n\n- For example, if **Y_t** is the current series and **Y_t-1** is the lag 1 of Y, then the partial autocorrelation of **lag 3 (Y_t-3)** is the coefficient $\\alpha_3$ of Y_t-3 in the following equation:","metadata":{}},{"cell_type":"markdown","source":"![Partial Autocorrelation Function](https://www.machinelearningplus.com/wp-content/uploads/2019/02/12_5_Autoregression_Equation-min.png?ezimgfmt=ng:webp/ngcb1)\n\nimage source : https://www.machinelearningplus.com/wp-content/uploads/2019/02/12_5_Autoregression_Equation-min.png?ezimgfmt=ng:webp/ngcb1","metadata":{}},{"cell_type":"markdown","source":"# **18. Lag Plots** <a class=\"anchor\" id=\"18\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- A **Lag plot** is a scatter plot of a time series against a lag of itself. It is normally used to check for autocorrelation. If there is any pattern existing in the series, the series is autocorrelated. If there is no such pattern, the series is likely to be random white noise.\n","metadata":{}},{"cell_type":"code","source":"from pandas.plotting import lag_plot\nplt.rcParams.update({'ytick.left' : False, 'axes.titlepad':10})\n\n# Plot\nfig, axes = plt.subplots(1, 4, figsize=(10,3), sharex=True, sharey=True, dpi=100)\nfor i, ax in enumerate(axes.flatten()[:4]):\n    lag_plot(df['Number of Passengers'], lag=i+1, ax=ax, c='firebrick')\n    ax.set_title('Lag ' + str(i+1))\n\nfig.suptitle('Lag Plots of Air Passengers', y=1.05)    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **19. Granger Causality Test** <a class=\"anchor\" id=\"19\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n- **Granger causality test** is used to determine if one time series will be useful to forecast another. It is based on the idea that if X causes Y, then the forecast of Y based on previous values of Y AND the previous values of X should outperform the forecast of Y based on previous values of Y alone.\n\n\n- So, **Granger causality test** should not be used to test if a lag of Y causes Y. Instead, it is generally used on exogenous (not Y lag) variables only. It is implemented in the statsmodel package.\n\n\n- It accepts a 2D array with 2 columns as the main argument. The values are in the first column and the predictor (X) is in the second column. The Null hypothesis is that the series in the second column, does not Granger cause the series in the first. If the P-Values are less than a significance level (0.05) then we reject the null hypothesis and conclude that the said lag of X is indeed useful. The second argument maxlag says till how many lags of Y should be included in the test.","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import grangercausalitytests\ndata = pd.read_csv('/kaggle/input/dataset/dataset.txt')\ndata['date'] = pd.to_datetime(data['date'])\ndata['month'] = data.date.dt.month\ngrangercausalitytests(data[['value', 'month']], maxlag=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In the above case, the p-values are zero for all tests. So the â€˜monthâ€™ indeed can be used to forecast the values.","metadata":{}},{"cell_type":"markdown","source":"# **20. Smoothening a Time Series** <a class=\"anchor\" id=\"20\"></a>\n\n\n[Table of Contents](#0.1)\n\n\n\nSmoothening of a time series may be useful in the following circumstances:\n\n\n- Reducing the effect of noise in a signal get a fair approximation of the noise-filtered series.\n- The smoothed version of series can be used as a feature to explain the original series itself.\n- Visualize the underlying trend better.\n\n\nWe can smoothen a time series using the following methods:\n\n\n- Take a moving average\n- Do a LOESS smoothing (Localized Regression)\n- Do a LOWESS smoothing (Locally Weighted Regression)","metadata":{}},{"cell_type":"markdown","source":"## **Moving Average** \n\n\n- **Moving average** is the average of a rolling window of defined width. We must choose the window-width wisely, because, large window-size will over-smooth the series. For example, a window-size equal to the seasonal duration (ex: 12 for a month-wise series), will effectively nullify the seasonal effect.\n ","metadata":{}},{"cell_type":"markdown","source":"## **Localized Regression**\n\n\n- LOESS, short for â€˜Localized Regressionâ€™ fits multiple regressions in the local neighborhood of each point. It is implemented in the statsmodels package, where you can control the degree of smoothing using frac argument which specifies the percentage of data points nearby that should be considered to fit a regression model.","metadata":{}},{"cell_type":"markdown","source":"# **21. References** <a class=\"anchor\" id=\"21\"></a>\n\n[Table of Contents](#0.1)\n\nThe concepts and code in this notebook is taken from the following websites:-\n\n1.\thttps://www.machinelearningplus.com/time-series/time-series-analysis-python/\n2.\thttps://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b\n3.\thttps://towardsdatascience.com/time-series-analysis-in-python-an-introduction-70d5a5b1d52a\n4.\thttps://www.digitalocean.com/community/tutorials/a-guide-to-time-series-visualization-with-python-3\n\n","metadata":{}},{"cell_type":"markdown","source":"So, now we will come to the end of this notebook.\n\nI hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n","metadata":{"trusted":true}},{"cell_type":"markdown","source":"[Go to Top](#0)","metadata":{}}]}